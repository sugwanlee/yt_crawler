# --------- í”„ë¡œì íŠ¸ì—ì„œ importí•œ ëª©ë¡ ---------------
from youtube_crawling.models import YouTubeVideo, YouTubeProduct
# --------- seleniumì—ì„œ importí•œ ëª©ë¡ ---------------
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# --------- webdriverì—ì„œ importí•œ ëª©ë¡ ---------------
from webdriver_manager.chrome import ChromeDriverManager
from contextlib import contextmanager # ë“œë¼ì´ë²„ ê´€ë¦¬í•˜ëŠ” íƒœê·¸
# --------- ê·¸ ì™¸ í¬ë¡¤ë§ ì½”ë“œë¥¼ ìœ„í•´ importí•œ ëª©ë¡ ---------------
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Union, Dict, Optional
from urllib.parse import urlparse, unquote, parse_qsl
from django.db import transaction
import pandas as pd
import logging, time, re, json, os, urllib.parse


# ----------------------------- â¬‡ï¸ logging ì„¤ì • -----------------------------

logger = logging.getLogger(__name__)  # logger.info(), logger.warning()ë§Œ ì¨ì•¼í•´ìš©

# --------- driver í•œ ë²ˆìœ¼ë¡œ ì •ì˜ ---------------
@contextmanager
def create_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--no-sandbox")           # ìƒŒë“œë°•ìŠ¤ ë¹„í™œì„±í™” (ë³´ì•ˆ ê¸°ëŠ¥ í•´ì œ)
    options.add_argument("--disable-dev-shm-usage")# ê³µìœ  ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„í™œì„±í™”
    options.add_argument("--disable-gpu")          # GPU í•˜ë“œì›¨ì–´ ê°€ì† ë¹„í™œì„±í™”
    options.add_argument("--disable-extensions")   # í¬ë¡¬ í™•ì¥ í”„ë¡œê·¸ë¨ ë¹„í™œì„±í™”
    options.add_argument("--disable-infobars")     # ì •ë³´ í‘œì‹œì¤„ ë¹„í™œì„±í™”
    options.add_argument("--start-maximized")      # ë¸Œë¼ìš°ì € ìµœëŒ€í™”
    options.add_argument("--disable-notifications")# ì•Œë¦¼ ë¹„í™œì„±í™”
    options.add_argument('--ignore-certificate-errors')  # ì¸ì¦ì„œ ì˜¤ë¥˜ ë¬´ì‹œ
    options.add_argument('--ignore-ssl-errors')    # SSL ì˜¤ë¥˜ ë¬´ì‹œ
    # User-Agent ì„¤ì •
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36')
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    logger.info("ğŸŸ¢ ChromeDriver ì‹¤í–‰")
    try:
        yield driver
    except Exception as e:
        logger.error(f"âŒ WebDriver ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        raise
    finally:
        driver.quit()
        logger.info("ğŸ›‘ ChromeDriver ì¢…ë£Œ")


# ---------------------- â¬‡ï¸ URL ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€ ----------------------
def clean_youtube_url(url: str) -> str:
    """YouTube URLì„ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # URLì—ì„œ 'watch?v=' ë¶€ë¶„ì´ ì¤‘ë³µë˜ëŠ”ì§€ í™•ì¸
        if url.count('watch?v=') > 1:
            # ë§ˆì§€ë§‰ 'watch?v=' ì´í›„ì˜ ë¶€ë¶„ë§Œ ê°€ì ¸ì˜´
            video_id = url.split('watch?v=')[-1]
            return f'https://www.youtube.com/watch?v={video_id}'
        return url
    except Exception as e:
        logger.error(f"âŒ URL ì •ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return url


# ----------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì±„ë„ì˜ ì˜ìƒ ì „ë¶€ ê°€ì§€ê³  ì˜¤ëŠ” í•¨ìˆ˜ -----------------------------
def get_all_video_ids(driver, channel_url):
    logger.info(f"ğŸ” ì±„ë„ ì˜ìƒ ID ìˆ˜ì§‘ ì‹œì‘: {channel_url}")

    try:
        videos_url = channel_url.rstrip('/') + "/videos"
        driver.get(videos_url)
        time.sleep(3)  # í˜ì´ì§€ ë¡œë”©ì„ ìœ„í•œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€

        video_urls = set()
        last_height = driver.execute_script("return document.documentElement.scrollHeight")
        
        SCROLL_PAUSE_TIME = 3
        MAX_RETRIES = 5
        retries = 0

        while True:
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´
            driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);")
            time.sleep(SCROLL_PAUSE_TIME)
            
            # ì˜ìƒ ë§í¬ ìˆ˜ì§‘
            elements = driver.find_elements(By.CSS_SELECTOR, 'a#video-title-link')
            for elem in elements:
                href = elem.get_attribute("href")
                if href and "watch?v=" in href:
                    # URL ì •ë¦¬ í•¨ìˆ˜ ì ìš©
                    cleaned_url = clean_youtube_url(href)
                    video_urls.add(cleaned_url)

            new_height = driver.execute_script("return document.documentElement.scrollHeight")
            if new_height == last_height:
                retries += 1
                if retries >= MAX_RETRIES:
                    break
            else:
                retries = 0
            last_height = new_height

        video_count = len(video_urls)
        if video_count > 0:
            logger.info(f"âœ… ì´ {video_count}ê°œì˜ ì˜ìƒ URL ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤")

        return list(video_urls)
    except Exception as e:
        logger.error(f"âŒ ì˜ìƒ ID ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return []

# ----------------------------- â¬‡ï¸ elementì˜ text ì¶”ì¶œí•˜ëŠ” ìœ í‹¸ í•¨ìˆ˜ -----------------------------
def safe_get_text(element, default=""):
    try:
        return element.text.strip()
    except Exception:
        return default


# ---------------------- â¬‡ï¸ ì¡°íšŒìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: ì¡°íšŒìˆ˜ 1,234íšŒ -> 1234) ----------------------
def parse_view_count(text: str) -> int:
    try:
        if not text:
            return 0
        # ì¡°íšŒìˆ˜ì™€ íšŒë¥¼ ì œê±°í•˜ê³  ìˆ«ìì™€ ì†Œìˆ˜ì , ë‹¨ìœ„(ë§Œ)ë§Œ ë‚¨ê¹€
        cleaned = text.replace("ì¡°íšŒìˆ˜", "").replace("íšŒ", "").replace(",", "").strip()
        
        # ë°± ë‹¨ìœ„ê°€ ìˆëŠ” ê²½ìš°
        if "ì²œ" in cleaned:
            number = float(cleaned.replace("ì²œ", ""))
            return int(number * 1000)
        # ë§Œ ë‹¨ìœ„ê°€ ìˆëŠ” ê²½ìš°
        elif "ë§Œ" in cleaned:
            number = float(cleaned.replace("ë§Œ", ""))
            return int(number * 10000)
        
        return int(cleaned)
    except ValueError as e:
        logger.warning(f"âš ï¸ ì¡°íšŒìˆ˜ íŒŒì‹± ì‹¤íŒ¨: '{text}', ì´ìœ : {e}")
        return 0


# -------------- â¬‡ï¸ êµ¬ë…ì ìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: 1.2ë§Œëª… -> 12000) ------------------
def parse_subscriber_count(text: str) -> int:
    try:
        if not text:
            return 0
        text = text.replace("êµ¬ë…ì", "").replace("ëª…", "").replace(",", "").strip()
        
        if "ì²œ" in text:
            number = float(text.replace("ì²œ", ""))
            return int(number * 1000)
        elif "ë§Œ" in text:
            number = float(text.replace("ë§Œ", ""))
            return int(number * 10000)
        elif "ì–µ" in text:
            number = float(text.replace("ì–µ", ""))
            return int(number * 100000000)
        
        return int(text) if text.strip().isdigit() else 0
    except Exception as e:
        logger.error(f"âŒ êµ¬ë…ì ìˆ˜ ë³€í™˜ ì‹¤íŒ¨: {text}, ì—ëŸ¬: {e}")
        return 0
    
    
# ---------------------- â¬‡ï¸ ê°€ê²© í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€ ----------------------
def parse_price(price_text: str) -> int:
    try:
        if not price_text or pd.isna(price_text):
            return 0
        # 'â‚©' ê¸°í˜¸ì™€ ì‰¼í‘œ ì œê±° í›„ ìˆ«ìë§Œ ì¶”ì¶œ
        cleaned_price = re.sub(r'[â‚©,\s]', '', price_text)
        # ìˆ«ìê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë³€í™˜
        if re.search(r'\d', cleaned_price):
            return int(re.sub(r'[^\d]', '', cleaned_price))
        return 0
    except Exception as e:
        logger.warning(f"âš ï¸ ê°€ê²© ë³€í™˜ ì‹¤íŒ¨: {price_text}, ì—ëŸ¬: {e}")
        return 0


# ---------------------- â¬‡ï¸ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ----------------------
def format_date(date_str: str) -> datetime:
    """ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
    try:
        if match := re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.?', date_str):
            year, month, day = match.groups()
            return datetime(int(year), int(month), int(day))
        elif match := re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', date_str):
            year, month, day = match.groups()
            return datetime(int(year), int(month), int(day))
        elif match := re.search(r'(\d{4})(\d{2})(\d{2})', date_str):
            year, month, day = match.groups()
            return datetime(int(year), int(month), int(day))
        else:
            logger.warning(f"âš ï¸ ë‚ ì§œ í˜•ì‹ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŒ: {date_str}, í˜„ì¬ ë‚ ì§œ ì‚¬ìš©")
            return datetime.now()
    except Exception as e:
        logger.error(f"âŒ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {date_str}, ì—ëŸ¬: {e}")
        return datetime.now()


# ---------------------- â¬‡ï¸ ì„¤ëª…ë€ì˜ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±° ----------------------
def clean_description(text: str) -> str:
    if not text:
        return ""
    # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ í†µì¼
    text = re.sub(r'\n\s*\n', '\n', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    return text.strip()


# ---------------------- â¬‡ï¸ ì œí’ˆ ê°œìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: 5ê°œ ì œí’ˆ) ----------------------
def parse_product_count(text: str) -> Union[int, None]:
    try:
        if match := re.search(r'(\d+)\s*ê°œ\s*ì œí’ˆ', text):
            return int(match.group(1))
    except:
        logger.warning(f"âš ï¸ ì œí’ˆ ê°œìˆ˜ ëª» ì°¾ì•˜ëŠ”ë…??")
    return None

# ---------------------- â¬‡ï¸ ë”ë³´ê¸° í´ë¦­ ë° ë”ë³´ê¸°ë€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ----------------------
def click_description(driver) -> str:
    try:
        # ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¼ìœ¼ë¡œì¨ ë²„íŠ¼ì´ ë¡œë“œë˜ë„ë¡ ìœ ë„
        body = driver.find_element(By.TAG_NAME, 'body')
        for _ in range(3):
            body.send_keys(Keys.END)
            time.sleep(1)
        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„ (2ê°€ì§€ selector)
        try:
            expand_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "#expand"))
            )
            driver.execute_script("arguments[0].click();", expand_button)
            logger.info("ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
        except Exception:
            logger.info("ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ ë˜ëŠ” í´ë¦­ ì‹¤íŒ¨, ë¬´ì‹œí•˜ê³  ì§„í–‰")

        selectors = [
            "#description-text-container",
            "#description-inline-expander"
        ]
        for selector in selectors:
            try:
                elem = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                desc = elem.text.strip()
                if desc:
                    return desc
            except Exception:
                logger.debug(f"'{selector}'ë¡œ ì„¤ëª…ë€ ì¶”ì¶œ ì‹¤íŒ¨, ë‹¤ìŒ ì‹œë„")
        logger.warning("ë”ë³´ê¸°ë€ì— ì„¤ëª…ì´ ì—†ìŒ")
        return "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
        
    except Exception as e:
        logger.error(f"âŒ ì„¤ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
        return "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
    
    
#--------------------------------------- ì œí’ˆ ì •ë³´ ì¶”ì¶œ -------------------------------------
def extract_products_from_dom(driver, soup: BeautifulSoup) -> list[dict]:
    products = []
    try:
        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„
        try:
            more_button = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "#expand"))
            )
            driver.execute_script("arguments[0].click();", more_button)
            logger.info("âœ… ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
            time.sleep(2)  # ì œí’ˆ ì •ë³´ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        except Exception as e:
            logger.info(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ (ì´ë¯¸ í¼ì³ì ¸ ìˆì„ ìˆ˜ ìˆìŒ): {e}")

        # ì œí’ˆ ì•„ì´í…œ ì°¾ê¸° - ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
        product_selectors = [
            "#items > ytd-merch-shelf-item-renderer",
            "ytd-merch-shelf-renderer #items ytd-merch-shelf-item-renderer",
            "#merch-shelf #items ytd-merch-shelf-item-renderer",
            "ytd-merch-shelf-renderer ytd-merch-shelf-item-renderer",
            "#product-items ytd-merch-shelf-item-renderer"
        ]
        
        product_items = []
        for selector in product_selectors:
            product_items = soup.select(selector)
            if product_items:
                logger.info(f"âœ… ì œí’ˆ ì•„ì´í…œ ì°¾ìŒ: {selector}")
                break
                
        total_items = len(product_items)
        logger.info(f"ì´ {total_items}ê°œì˜ ì œí’ˆ ì•„ì´í…œì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

        '''250526 ì œí’ˆ ì •ë³´ ì¶”ì¶œ ìˆ˜ì •'''
        for item in product_items:
            try:
                product_info = {}

                # ì œí’ˆëª… ì¶”ì¶œ - ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
                title_selectors = [
                    ".product-item-title",
                    ".title",
                    "h3",
                    "yt-formatted-string.title"
                ]
                
                title_text = None
                for selector in title_selectors:
                    title_elem = item.select_one(selector)
                    if title_elem and (title_text := title_elem.get_text(strip=True)):
                        product_info["title"] = title_text
                        logger.info(f"âœ… ì œí’ˆëª… ì¶”ì¶œ ì„±ê³µ: {title_text}")
                        break
                
                if not title_text:
                    logger.warning("âš ï¸ ì œí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹¤ìŒ ì•„ì´í…œìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤")
                    continue

                # ì œí’ˆ ë§í¬ ì¶”ì¶œ - ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
                link_selectors = [
                    ".product-item-description",
                    "a.yt-simple-endpoint",
                    "a[href]"
                ]
                
                product_url = None
                for selector in link_selectors:
                    link_elem = item.select_one(selector)
                    if link_elem:
                        if 'href' in link_elem.attrs:
                            product_url = link_elem['href']
                        else:
                            product_url = link_elem.get_text(strip=True)
                        if product_url:
                            product_info["url"] = product_url
                            logger.info(f"âœ… ì œí’ˆ ë§í¬ ì¶”ì¶œ ì„±ê³µ: {product_url}")
                            break

                # ê°€ê²© ì¶”ì¶œ - ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
                price_selectors = [
                    ".product-item-price",
                    ".price",
                    "span.price",
                    "yt-formatted-string.price"
                ]
                
                price_text = None
                for selector in price_selectors:
                    price_elem = item.select_one(selector)
                    if price_elem and (price_text := price_elem.get_text(strip=True)):
                        product_info["price"] = price_text
                        logger.info(f"âœ… ì œí’ˆ ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {price_text}")
                        break
                
                if not price_text:
                    logger.warning("âš ï¸ ê°€ê²© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹¤ìŒ ì•„ì´í…œìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤")
                    continue

                # 250527 ì´ë¯¸ì§€ URL ì¶”ì¶œ - ëª¨ë“  ì´ë¯¸ì§€ URLì—ì„œ shopping? í¬í•¨ëœ ê²ƒë§Œ í•„í„°ë§
                try:
                    # ëª¨ë“  ì´ë¯¸ì§€ URL ìˆ˜ì§‘
                    all_img_urls = []
                    for img in soup.find_all('img'):
                        src = img.get('src', '')
                        if 'shopping?' in src:
                            all_img_urls.append(src)
                    
                    # í˜„ì¬ ì œí’ˆì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ URL ì°¾ê¸°
                    if all_img_urls:
                        product_info["imageUrl"] = all_img_urls[0]
                        logger.info(f"âœ… ì‡¼í•‘ ì´ë¯¸ì§€ URL ì¶”ì¶œ ì„±ê³µ: {all_img_urls[0]}")
                    else:
                        logger.warning("âš ï¸ shopping?ì„ í¬í•¨í•œ ì´ë¯¸ì§€ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        product_info["imageUrl"] = ""
                        
                except Exception as e:
                    logger.error(f"âŒ ì´ë¯¸ì§€ URL ì¶”ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
                    product_info["imageUrl"] = ""

                # íŒë§¤ì²˜ ì¶”ì¶œ - ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
                merchant_selectors = [
                    ".product-item-merchant-text",
                    ".merchant",
                    "span.merchant",
                    "yt-formatted-string.merchant"
                ]
                
                merchant_text = None
                for selector in merchant_selectors:
                    merchant_elem = item.select_one(selector)
                    if merchant_elem and (merchant_text := merchant_elem.get_text(strip=True)):
                        merchant_name = merchant_text.replace("!", "").strip()
                        product_info["merchant"] = merchant_name
                        logger.info(f"âœ… íŒë§¤ì²˜ ì¶”ì¶œ ì„±ê³µ: {merchant_name}")
                        break

                # ì œí’ˆëª…ê³¼ ê°€ê²©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                if "title" in product_info and "price" in product_info:
                    products.append(product_info)
                    logger.info(f"âœ… ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {product_info['title']} ({product_info['price']})")

            except Exception as e:
                logger.error(f"âŒ ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
                continue

        logger.info(f"ì´ {len(products)}ê°œì˜ ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
        return products

    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ì œí’ˆ ì¶”ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return []

# ---------------------- â¬‡ï¸ ì˜ìƒ ê¸°ë³¸ ì •ë³´: ì œëª©, ì±„ë„ëª…, êµ¬ë…ì ìˆ˜, ì¡°íšŒìˆ˜, ì—…ë¡œë“œì¼, ì œí’ˆ ê°œìˆ˜ ----------------------
def base_youtube_info(driver, video_url: str) -> pd.DataFrame:
    logger.info("Crawling video: %s", video_url)
    today_str = datetime.today().strftime('%Y%m%d')

    try:
        driver.get(video_url)
        # 250525 í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        time.sleep(5)  # 3ì´ˆì—ì„œ 5ì´ˆë¡œ ì¦ê°€
        
        # í˜ì´ì§€ ìŠ¤í¬ë¡¤ì„ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•˜ì—¬ ë™ì  ì»¨í…ì¸  ë¡œë“œ
        for _ in range(5):  # 3íšŒì—ì„œ 5íšŒë¡œ ì¦ê°€
            driver.execute_script("window.scrollTo(0, window.scrollY + 500);")
            time.sleep(3)  # 2ì´ˆì—ì„œ 3ì´ˆë¡œ ì¦ê°€
        
        wait = WebDriverWait(driver, 20)
        
        # 250523 ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
        expand_button_selectors = [
            "tp-yt-paper-button#expand", "#expand", "#expand-button", "#more",
            "ytd-button-renderer#more", "ytd-expander#description [aria-label='ë”ë³´ê¸°']",
            "ytd-expander[description-collapsed] #expand"
        ]
        
        for selector in expand_button_selectors:
            try:
                more_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))
                driver.execute_script("arguments[0].click();", more_button)
                logger.info(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ: {selector}")
                time.sleep(3)  # ë”ë³´ê¸° í´ë¦­ í›„ ì»¨í…ì¸  ë¡œë“œ ëŒ€ê¸°
                break
            except:
                continue
                
        # 250523 ì œí’ˆ ì„¹ì…˜ ì„ íƒì§€ ì¶”ê°€
        product_selectors = [
            "ytd-product-metadata-badge-renderer", "ytd-merch-shelf-renderer",
            "ytd-product-item-renderer","#product-shelf", "#product-list",
            "ytd-merch-product-renderer", "#product-items", ".product-item",
            "#content ytd-metadata-row-container-renderer",
            "ytd-metadata-row-renderer", "#product-section"
        ]
        
        for selector in product_selectors:
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                logger.info(f"ì œí’ˆ ì„¹ì…˜ ì°¾ìŒ: {selector}")
                break
            except:
                continue
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        soup_file_path = "/Users/mac/Desktop/minmin/intern/crawling_auto_code/soup_files"
        
        # soup_files ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        if not os.path.exists(soup_file_path):
            os.makedirs(soup_file_path)
            
        # í˜„ì¬ ë‚ ì§œë¥¼ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
        today_str = datetime.now().strftime("%y%m%d")
        
        # ì˜¤ëŠ˜ ë‚ ì§œì˜ íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ê°€ì¥ í° ë²ˆí˜¸ ì°¾ê¸°
        existing_files = [f for f in os.listdir(soup_file_path) if f.endswith(f"_{today_str}.txt")]
        current_number = 1

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        video_id = video_url.split("v=")[-1]
        
        '''250522 ì œëª© (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)'''
        title_selectors = [
            "h1.title yt-formatted-string",
            "h1.title",
            "#title h1",
            "#container h1.style-scope.ytd-watch-metadata"
        ]
        title = None
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if title:
                    break
        title = title or "ì œëª© ì—†ìŒ"
        logger.info(f"ì œëª©: {title}")

        '''250522 ì±„ë„ëª… (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)'''
        channel_selectors = [
            "ytd-channel-name yt-formatted-string#text a",
            "ytd-channel-name a",
            "#channel-name a"
        ]
        channel_name = None
        for selector in channel_selectors:
            channel_tag = soup.select_one(selector)
            if channel_tag:
                channel_name = channel_tag.text.strip()
                break
        channel_name = channel_name or "ì±„ë„ ì—†ìŒ"

        '''250522 êµ¬ë…ì ìˆ˜'''
        sub_selectors = [
            "yt-formatted-string#owner-sub-count",
            "#subscriber-count"
        ]
        subscriber_count = None
        for selector in sub_selectors:
            sub_tag = soup.select_one(selector)
            if sub_tag:
                subscriber_count = sub_tag.text.strip()
                break
        subscriber_count = subscriber_count or "êµ¬ë…ì ìˆ˜ ì—†ìŒ"

        '''250522 ì¡°íšŒìˆ˜'''
        view_selectors = [
            "span.view-count",
            "#view-count",
            "ytd-video-view-count-renderer"
        ]
        view_count = None
        for selector in view_selectors:
            view_tag = soup.select_one(selector)
            if view_tag:
                view_count = view_tag.text.strip()
                break
        view_count = view_count or "ì¡°íšŒìˆ˜ ì—†ìŒ"

        '''250522 ì—…ë¡œë“œì¼'''
        date_selectors = [
            "#info-strings yt-formatted-string",
            "#upload-info .date",
            "ytd-video-primary-info-renderer yt-formatted-string.ytd-video-primary-info-renderer:not([id])"
        ]
        upload_date = None
        for selector in date_selectors:
            date_tag = soup.select_one(selector)
            if date_tag:
                upload_date = date_tag.text.strip()
                break
        upload_date = upload_date or "ë‚ ì§œ ì—†ìŒ"

        '''250522 ì„¤ëª…ë€'''
        desc_selectors = [
            "ytd-expander#description yt-formatted-string",
            "#description",
            "#description-inline-expander",
            "#description-text-container"
        ]
        description = None
        for selector in desc_selectors:
            desc_tag = soup.select_one(selector)
            if desc_tag:
                description = desc_tag.text.strip()
                if description:
                    break
        description = description or "ì„¤ëª… ì—†ìŒ"
        logger.info(f"ì„¤ëª… ê¸¸ì´: {len(description)} ê¸€ì")

        '''250522 ì œí’ˆ ê°œìˆ˜'''
        # HTMLì—ì„œ ì œí’ˆ ê°œìˆ˜ ì§ì ‘ ì¶”ì¶œ ì‹œë„
        try:
            product_count_elem = soup.select_one("yt-formatted-string#info")
            if product_count_elem:
                text_content = product_count_elem.get_text()
                # "nê°œ ì œí’ˆ" íŒ¨í„´ ì°¾ê¸°
                if match := re.search(r'(\d+)ê°œ\s*ì œí’ˆ', text_content):
                    product_count = int(match.group(1))
                    logger.info(f"âœ… HTMLì—ì„œ ì œí’ˆ ê°œìˆ˜ ì¶”ì¶œ ì„±ê³µ: {product_count}ê°œ")
                else:
                    product_count = 0
                    logger.warning("âš ï¸ HTMLì—ì„œ ì œí’ˆ ê°œìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                product_count = 0
                logger.warning("âš ï¸ ì œí’ˆ ê°œìˆ˜ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        except Exception as e:
            logger.error(f"âŒ HTMLì—ì„œ ì œí’ˆ ê°œìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            product_count = 0

        # ì œí’ˆ ì •ë³´ ì¶”ì¶œ
        products = extract_products_from_dom(driver, soup)
        if products is None:
            products = []
            
        # HTMLì—ì„œ ì¶”ì¶œí•œ ì œí’ˆ ê°œìˆ˜ê°€ 0ì´ê³ , ì‹¤ì œ ì œí’ˆì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹¤ì œ ê°œìˆ˜ ì‚¬ìš©
        if product_count == 0 and products:
            product_count = len(products)
            logger.info(f"âœ… ì‹¤ì œ ì¶”ì¶œëœ ì œí’ˆ ê°œìˆ˜ ì‚¬ìš©: {product_count}ê°œ")
        
        logger.info(f"âœ… ìµœì¢… ì œí’ˆ ê°œìˆ˜: {product_count}ê°œ")

        # ê¸°ë³¸ ë°ì´í„° ì„¸íŠ¸
        base_data = []
        
        # 250523 ì œí’ˆì´ ìˆëŠ” ê²½ìš°, ê° ì œí’ˆë³„ë¡œ row ìƒì„±
        if products:
            for product in products:
                row_data = {
                    "youtube_id": video_id,
                    "title": title,
                    "channel_name": channel_name,
                    "subscribers": subscriber_count,
                    "view_count": view_count,
                    "upload_date": upload_date,
                    "extracted_date": today_str,
                    "video_url": video_url,
                    "description": description,
                    "product_count": product_count,  # HTMLì—ì„œ ì¶”ì¶œí•œ ì œí’ˆ ê°œìˆ˜ ì‚¬ìš©
                    "product_name": product.get("title", ""),
                    "product_price": product.get("price", ""),
                    "product_image_url": product.get("imageUrl", ""),
                    "product_merchant_url": product.get("url", ""),
                    "product_merchant": product.get("merchant", "")
                }
                base_data.append(row_data)
        else:
            # ì œí’ˆì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥
            base_data.append({
                "youtube_id": video_id,
                "title": title,
                "channel_name": channel_name,
                "subscribers": subscriber_count,
                "view_count": view_count,
                "upload_date": upload_date,
                "extracted_date": today_str,
                "video_url": video_url,
                "description": description,
                "product_count": product_count,
                "product_name": "",
                "product_price": "",
                "product_image_url": "",
                "product_merchant_url": "",
                "product_merchant": ""
            })

        logger.info(f"ğŸ“¦ ìˆ˜ì§‘ëœ ë°ì´í„° í–‰ ê°œìˆ˜: {len(base_data)}")
        return pd.DataFrame(base_data)
    
    except Exception as e:
        logger.error(f"âŒ base_youtube_info ì˜ˆì™¸: {e}", exc_info=True)
        return pd.DataFrame()

# ----------------------------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì˜ìƒ URL ì ‘ì† í›„ ë°ì´í„° ìˆ˜ì§‘ ìˆ˜í–‰ -----------------------------------------------

def collect_video_data(driver, video_id: str, index: int = None, total: int = None) -> pd.DataFrame:
    # URL ì •ë¦¬
    base_url = clean_youtube_url(f"https://www.youtube.com/watch?v={video_id}")
    
    try:
        driver.get(base_url)
        if index is not None and total is not None:
            logger.info(f"\nğŸ“¹ ({index}/{total}) í¬ë¡¤ë§ ì¤‘: {video_id}")

        df = base_youtube_info(driver, base_url)

        logger.info(f"ğŸ“¦ ìˆ˜ì§‘ëœ ì œí’ˆ ê°œìˆ˜: {len(df)}")
        if df.empty:
            logger.warning(f"âš ï¸ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŒ: {video_id}")

        return df
    
    except Exception as e:
            logger.error(f"âŒ ì˜ˆì™¸ ë°œìƒ - collect_video_data(): {video_id} | ì—ëŸ¬: {e}")
            return None

# ------------------------------------- â¬‡ï¸ í¬ë¡¤ë§ëœ ìœ íŠœë¸Œ ì˜ìƒì„ ì¡°íšŒí•˜ê³  ìˆ˜ì •í•˜ëŠ” ì½”ë“œ ------------------------------
def update_youtube_data_to_db(dataframe: pd.DataFrame) -> int:
    if dataframe.empty:
        return 0

    video_id = dataframe.iloc[0]['video_id']
    
    try:
        video = YouTubeVideo.objects.get(video_id=video_id)
        row = dataframe.iloc[0]

        # ê¸°ì¡´ ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸
        video.extracted_date = row['extracted_date']
        video.upload_date = row['upload_date']
        video.channel_name = row['channel_name']
        video.subscriber_count = row['subscriber_count']
        video.video_url = row['video_url']
        video.title = row['title']
        video.view_count = row['view_count']
        video.product_count = row['product_count']
        video.description = row['description']
        video.save()

        # ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì‚­ì œ í›„ ìƒˆë¡œ ì €ì¥
        video.products.all().delete()

        # pd.DataFrame == dataframe
        for _, row in dataframe.iterrows():
            product_name = row.get('product_name')
            if product_name and pd.notna(product_name):
                product, created = YouTubeProduct.objects.update_or_create(
                    video=video,
                    product_name=row.get('title', 'ì œí’ˆ ì—†ìŒ'),
                    defaults={
                        "product_price": row.get('price'),
                        "product_image_link": row.get('imageUrl'),
                        "product_merchant_link": row.get('url'),
                        "product_merchant": row.get('merchant', '')
                    }
                )
        logger.info(f"ğŸ” ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {video_id}")
        return 1

    except YouTubeVideo.DoesNotExist:
        logger.warning(f"âŒ í•´ë‹¹ video_idì— ëŒ€í•œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤: {video_id}")
        return 0

# ------------------------------------- â¬‡ï¸ ì±„ë„ URLì—ì„œ ê³ ìœ í•œ ID ì¶”ì¶œ (ì˜ˆ: UCxxxx ë˜ëŠ” @handle í˜•ì‹) ------------------------------
def get_channel_id_from_url(channel_url):
    parsed = urlparse(channel_url)
    parts = parsed.path.strip("/").split("/")
    return parts[-1] if parts else "unknown_channel"

# ------------------------------------- â¬‡ï¸ ì±„ë„ ì´ë¦„ì„ YouTube ì±„ë„ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜´ ------------------------------
def get_channel_name(driver, channel_url):
    driver.get(channel_url)
    driver.implicitly_wait(5)
    try:
        title_element = driver.find_element("xpath", '//meta[@property="og:title"]')
        channel_name = title_element.get_attribute("content")
        # URL ë””ì½”ë”©ëœ ì±„ë„ëª… ë°˜í™˜
        decoded_name = urllib.parse.unquote(channel_name)
        return decoded_name  # slugify ì œê±°í•˜ì—¬ í•œê¸€ ìœ ì§€
    except Exception as e:
        logger.warning(f"âš ï¸ ì±„ë„ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return "unknown_channel"

# ------------------------------------- â¬‡ï¸ URL ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ë¦¬ ------------------------------
def validate_url(url: str) -> str:
    try:
        if not url:
            return ""
        # URL ìŠ¤í‚¤ë§ˆê°€ ì—†ëŠ” ê²½ìš° ì¶”ê°€
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        # URL ì¸ì½”ë”©
        return urllib.parse.quote(url, safe=':/?=&')
    except Exception as e:
        logger.error(f"âŒ URL ê²€ì¦ ì‹¤íŒ¨: {url}, ì—ëŸ¬: {e}")
        return ""
    
# ------------------------------------- â¬‡ï¸ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def save_to_db(data: pd.DataFrame):
    if data is None or data.empty:
        logger.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    saved_count = 0
    updated_count = 0
    
    try:
        with transaction.atomic():
            for video_id, video_group in data.groupby('youtube_id'):
                if not video_id:
                    logger.warning("âš ï¸ video_id ì—†ìŒ, ê±´ë„ˆëœë‹ˆë‹¤")
                    continue

                first_row = video_group.iloc[0]
                
                try:
                    # ë‚ ì§œ ë³€í™˜
                    extracted_date = format_date(first_row.get("extracted_date", ""))
                    upload_date = format_date(first_row.get("upload_date", ""))
                    
                    # ìˆ«ì ë°ì´í„° ë³€í™˜
                    subscriber_count = parse_subscriber_count(first_row.get("subscribers", "0"))
                    view_count = parse_view_count(first_row.get("view_count", "0"))
                    
                    # HTMLì—ì„œ ì¶”ì¶œí•œ ì œí’ˆ ê°œìˆ˜ ì‚¬ìš© (ì²« ë²ˆì§¸ í–‰ì—ì„œë§Œ ê°€ì ¸ì˜´)
                    product_count = int(first_row.get("product_count", 0))
                    logger.info(f"âœ… ë¹„ë””ì˜¤ {video_id}ì˜ ì œí’ˆ ê°œìˆ˜: {product_count}ê°œ")
                    
                    # URL ê²€ì¦
                    video_url = validate_url(first_row.get("video_url", ""))
                    
                    # ì˜ìƒ ì •ë³´ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
                    video_obj, created = YouTubeVideo.objects.update_or_create(
                        video_id=video_id,
                        defaults={
                            "extracted_date": extracted_date,
                            "upload_date": upload_date,
                            "channel_name": first_row.get("channel_name", ""),
                            "subscriber_count": subscriber_count,
                            "title": first_row.get("title", ""),
                            "view_count": view_count,
                            "video_url": video_url,
                            "product_count": product_count,  # HTMLì—ì„œ ì¶”ì¶œí•œ ì œí’ˆ ê°œìˆ˜ ì‚¬ìš©
                            "description": clean_description(first_row.get("description", "")),
                        }
                    )

                    if created:
                        logger.info(f"âœ¨ ìƒˆë¡œìš´ ì˜ìƒ ìƒì„±: {video_id}")
                    else:
                        logger.info(f"ğŸ”„ ê¸°ì¡´ ì˜ìƒ ì—…ë°ì´íŠ¸: {video_id}")
                        updated_count += 1

                    # ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì‚­ì œ
                    video_obj.products.all().delete()
                    
                    # ì œí’ˆ ì •ë³´ ì²˜ë¦¬
                    for _, row in video_group.iterrows():
                        product_name = row.get("product_name", "").strip()
                        if product_name:
                            try:
                                price = parse_price(row.get("product_price", "0"))
                                product_image_link = validate_url(row.get("product_image_url", ""))
                                product_merchant_link = validate_url(row.get("product_merchant_url", ""))
                                
                                product, created = YouTubeProduct.objects.update_or_create(
                                    video=video_obj,
                                    product_name=product_name,
                                    defaults={
                                        "product_price": price,
                                        "product_image_link": product_image_link,
                                        "product_merchant": row.get("product_merchant", ""),
                                        "product_merchant_link": product_merchant_link
                                    }
                                )
                                saved_count += 1
                                if created:
                                    logger.info(f"âœ¨ ìƒˆë¡œìš´ ì œí’ˆ ì •ë³´ ì €ì¥: {product_name} (ê°€ê²©: {price:,}ì›)")
                                else:
                                    logger.info(f"ğŸ”„ ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì—…ë°ì´íŠ¸: {product_name} (ê°€ê²©: {price:,}ì›)")
                            except Exception as e:
                                logger.error(f"âŒ ì œí’ˆ ì •ë³´ ì €ì¥ ì¤‘ ì—ëŸ¬ ë°œìƒ ({product_name}): {e}")
                                continue
                except Exception as e:
                    logger.error(f"âŒ ì˜ìƒ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ ({video_id}): {e}")
                    continue

    except Exception as e:
        logger.error(f"âŒ DB ì €ì¥ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}", exc_info=True)
        return 0

    logger.info(f"âœ… ì´ {updated_count}ê°œì˜ ì˜ìƒì´ ì—…ë°ì´íŠ¸ë˜ì—ˆê³ , {saved_count}ê°œì˜ ì œí’ˆì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return saved_count

# ------------------------------------- â¬‡ï¸ CSVìš©ìœ¼ë¡œ ë°ì´í„° ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df['view_count'] = df['view_count'].apply(parse_view_count)
    df['subscribers'] = df['subscribers'].apply(parse_subscriber_count)
    df['product_price'] = df['product_price'].apply(parse_price)
    df['description'] = df['description'].apply(clean_description)
    df['upload_date'] = df['upload_date'].apply(format_date)
    df['extracted_date'] = df['extracted_date'].apply(format_date)
    return df

# ------------------------------------- â¬‡ï¸ CSVë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ------------------------------
# def save_to_csv(df: pd.DataFrame, directory: str, channel_name: str) -> str:
#     df = preprocess_df(df)
#     try:
#         today_str = datetime.now().strftime("%Y%m%d")
        
#         # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
#         os.makedirs(directory, exist_ok=True)

#         # URL ì¸ì½”ë”©ëœ ì±„ë„ëª…ì„ ë””ì½”ë”©
#         decoded_channel_name = urllib.parse.unquote(channel_name)
        
#         # ì±„ë„ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ê³  ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
#         safe_channel_name = "".join(c for c in decoded_channel_name.replace(" ", "_") if c.isalnum() or c in ('_',)).rstrip()

#         # í•´ë‹¹ ë‚ ì§œì™€ ì±„ë„ ì´ë¦„ì˜ ê¸°ì¡´ íŒŒì¼ í™•ì¸
#         pattern = f"[0-9]{{2}}_{safe_channel_name}_{today_str}.csv"
#         existing_files = [
#             f for f in os.listdir(directory)
#             if re.match(pattern, f)
#         ]

#         # ë‹¤ìŒ ë²ˆí˜¸ ê²°ì •
#         if not existing_files:
#             next_number = 1
#         else:
#             numbers = []
#             for f in existing_files:
#                 try:
#                     num = int(f.split('_')[0])
#                     numbers.append(num)
#                 except (ValueError, IndexError):
#                     continue
#             next_number = max(numbers, default=0) + 1

#         # íŒŒì¼ëª… ìƒì„±
#         file_name = f"{next_number:02d}_{safe_channel_name}_{today_str}.csv"
#         file_path = os.path.join(directory, file_name)

#         # CSV ì €ì¥
#         df.to_csv(file_path, index=False, encoding='utf-8-sig')
#         logger.info(f"ğŸ’¾ CSV ì €ì¥ ì™„ë£Œ: {file_path}")
#         return file_path
        
#     except Exception as e:
#         logger.error(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
#         return None


# ------------------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì±„ë„ì˜ ì „ì²´ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def crawl_channel_videos(channel_url: str):
    with create_driver() as driver:
        video_ids = get_all_video_ids(driver, channel_url)

        total = len(video_ids)
        if total == 0:
            logger.warning("âŒ ì±„ë„ì—ì„œ ìˆ˜ì§‘ëœ ì˜ìƒ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ì´ {total}ê°œ ì˜ìƒ í¬ë¡¤ë§ ì‹œì‘")
        all_data = pd.DataFrame()

        for i, video_id in enumerate(video_ids, start=1):
            try:
                logger.info(f"\nğŸ” ({i}/{total}) ì˜ìƒ í¬ë¡¤ë§ ì‹œì‘: {video_id}")
                df = collect_video_data(driver, video_id)
                if df is not None and not df.empty:
                    all_data = pd.concat([all_data, df], ignore_index=True)
                    save_to_db(df)
                    logger.info(f"âœ… ({i}/{total}) ì˜ìƒ í¬ë¡¤ë§ ì™„ë£Œ: {video_id}")
            except Exception as e:
                logger.error(f"âŒ ({i}/{total}) ì˜ìƒ í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ë°œìƒ: {video_id}, ì—ëŸ¬: {e}", exc_info=True)

        # if not all_data.empty:
        #     try:
        #         # ì±„ë„ëª… ê°€ì ¸ì˜¤ê¸°
        #         channel_name = get_channel_name(driver, channel_url)
                
        #         # CSV ì €ì¥
        #         csv_path = save_to_csv(all_data, save_path, channel_name)
        #         if csv_path:
        #             logger.info(f"âœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
        #         else:
        #             logger.error("âŒ CSV íŒŒì¼ ì €ì¥ ì‹¤íŒ¨")
        #     except Exception as e:
        #         logger.error(f"âŒ CSV ì €ì¥ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}", exc_info=True)
        # else:
        #     logger.warning("âš ï¸ í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„° ì—†ìŒ")

# ------------------------------------- â¬‡ï¸ í¬ë¡¤ë§ ë©”ì¸ ì‹¤í–‰ë¶€ ------------------------------
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    urls = [
        "https://www.youtube.com/@%EC%B9%A1%EC%B4%89",
    ]

    for url in urls:
        try:
            logger.info(f"ğŸš€ ì±„ë„ í¬ë¡¤ë§ ì‹œì‘: {url}")
            crawl_channel_videos(url)
            logger.info(f"âœ… ì±„ë„ í¬ë¡¤ë§ ì™„ë£Œ: {url}")

        except Exception as e:
            logger.warning(f"âŒ ì±„ë„ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {url} - {e}")
        
        time.sleep(1)  # ê° ì±„ë„ ê°„ 1ì´ˆ ì‰¬ì—ˆë‹¤ê°€ ë‹¤ìŒ ì±„ë„ ì‹¤í–‰
